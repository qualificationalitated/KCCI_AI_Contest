"""
â— í…ìŠ¤íŠ¸ ì…ë ¥: ë³´ë„ìë£Œ ë˜ëŠ” ì •ì±…ë³´ê³ ì„œë¥¼ ì…ë ¥ë°›ëŠ” ê°„ë‹¨í•œ ì¸í„°í˜ì´ìŠ¤ ì œê³µ
â— ìë™ ìš”ì•½: AI ê¸°ë°˜ì˜ í…ìŠ¤íŠ¸ ìë™ ìš”ì•½ ì„œë¹„ìŠ¤ ì œê³µ (Transformers ëª¨ë¸ ~ llm ëª»ì“°ë‚˜? ~ í”„ë¡¬í”„íŠ¸ í”„ë¡œê·¸ë˜ë° ì—°ê³„ í•˜ê³ ì‹¶ì€ë°)
â— ê°ì„± ë¶„ì„: ìë£Œê°€ ê°–ëŠ” ê¸ì •ì , ë¶€ì •ì  ê°ì • í†¤ì„ ìë™ íŒë³„
â— ë¶„ì•¼ë³„ ì˜í–¥ë„ ì‹œê°í™”: ì‚°ì—… ë¶„ì•¼(ì˜ˆ: ë…¸ë™, ì—ë„ˆì§€, ê¸ˆìœµ ë“±)ì— ëŒ€í•œ ì˜í–¥ì„ íˆíŠ¸ë§µ í˜•ì‹ìœ¼ë¡œ ì‹œê°í™” ì œê³µ
â— ì‹œì‚¬ì  ì œê³µ: ë¶„ì„ ê²°ê³¼ ê¸°ë°˜ìœ¼ë¡œ ì‚¬íšŒì , ì‚°ì—…ì  ëŒ€ì‘ ë°©ì•ˆê³¼ ì£¼ìš” ì‹œì‚¬ì  ì œì‹œ
â— ê²€ìˆ˜ì ì˜ê²¬ ì‹œë®¬ë ˆì´ì…˜ : ê·¸ê·¸, ê²€ìˆ˜ì ì˜ê²¬ ë‚´ëŠ”ê²ƒë„ ë“¤ì–´ê°€ë©´ ì¢‹ì„ ë“¯
    ê²€ìˆ˜ì í˜ë¥´ì†Œë‚˜ë¥¼ ì„¤ì •í•´ë‘” í›„, í•´ë‹¹ ë³´ê³ ìë£Œê°€ ì˜ ë§Œë“¤ì–´ì¡ŒëŠ”ì§€, ì–´ë–¤ ë¶€ë¶„ì„ ì¶”ê°€í•˜ë©´ ì¢‹ì„ì§€ ì•Œë ¤ì£¼ëŠ” ê¸°ëŠ¥ ìˆìœ¼ë©´ ì¢‹ê² ìŒ
"""

import streamlit as st
from transformers import BartForConditionalGeneration, PreTrainedTokenizerFast, AutoTokenizer
from transformers import pipeline as hf_pipeline
import plotly.express as px
import pandas as pd
import random
import re
from collections import defaultdict
from sklearn.feature_extraction.text import TfidfVectorizer
import requests
import openai
from openai import OpenAI

# âœ… Streamlit í˜ì´ì§€ ì„¤ì •
st.set_page_config(page_title="KCCI ì‚°ì—…ë³„ ê°ì„± ì‹œë®¬ë ˆì´í„°", layout="wide")
st.title("ğŸ“Š ì‚°ì—…ë³„ ê°ì„± + ì˜í–¥ë„ ë¶„ì„ ì‹œë®¬ë ˆì´í„°")
st.markdown("ë³´ë„ìë£Œê°€ ì‚°ì—…ë³„ë¡œ ì–´ë–¤ ì •ì„œì™€ ì˜í–¥ì„ ì£¼ëŠ”ì§€ ìë™ ë¶„ì„í•˜ê³  ì‹œê°í™”í•©ë‹ˆë‹¤.")

# âœ… Chatgpt API ì„¤ì •
openai.api_key = st.secrets.get("openai_api_key", "")
oai_client = OpenAI(api_key=st.secrets.get("openai_api_key", ""))

# âœ… ëª¨ë¸ ë¡œë“œ ë° ìºì‹±
@st.cache_resource
def load_models():
    summarizer_tokenizer = PreTrainedTokenizerFast.from_pretrained(
        "digit82/kobart-summarization",
        bos_token='</s>', eos_token='</s>', unk_token='<unk>', pad_token='<pad>', mask_token='<mask>'
    )
    summarizer_model = BartForConditionalGeneration.from_pretrained("digit82/kobart-summarization")
    sentiment_analyzer = hf_pipeline("sentiment-analysis")
    sentiment_tokenizer = AutoTokenizer.from_pretrained("distilbert-base-uncased")
    return summarizer_tokenizer, summarizer_model, sentiment_analyzer, sentiment_tokenizer

tokenizer, model, sentiment_analyzer, sentiment_tokenizer = load_models()

SECTOR_KEYWORDS = {
    # ğŸ¯ ê²½ì œÂ·ì‚°ì—… ì „ë°˜
    "ë…¸ë™ì‹œì¥": ["ê³ ìš©", "ë…¸ë™", "ì„ê¸ˆ", "ì¼ìë¦¬", "ê·¼ë¡œì", "ê·¼ë¬´", "ì§ì¥"],
    "ê¸ˆìœµ": ["ê¸ˆë¦¬", "ëŒ€ì¶œ", "ì€í–‰", "ìë³¸", "íˆ¬ì", "ì¦ê¶Œ", "ê¸ˆìœµì‹œì¥", "ì±„ê¶Œ"],
    "ë¶€ë™ì‚°": ["ê±´ì„¤", "ë¶€ë™ì‚°", "ê³µê¸‰", "ì²­ì•½", "í† ì§€", "ì¬ê±´ì¶•", "ì„ëŒ€", "ë§¤ë§¤"],
    "ì—ë„ˆì§€Â·í™˜ê²½": ["ì—ë„ˆì§€", "íƒ„ì†Œ", "ê¸°í›„", "ì˜¨ì‹¤ê°€ìŠ¤", "ì¬ìƒì—ë„ˆì§€", "í™˜ê²½", "ì˜¤ì—¼", "ì§€ì†ê°€ëŠ¥"],
    "ë¬´ì—­Â·í†µìƒ": ["ìˆ˜ì¶œ", "ìˆ˜ì…", "ê´€ì„¸", "ë¬´ì—­", "í†µìƒ", "FTA", "WTO", "ë¬´ì—­ì ì"],
    "ì¤‘ì†Œê¸°ì—…Â·ì†Œìƒê³µì¸": ["ì¤‘ì†Œê¸°ì—…", "ì†Œìƒê³µì¸", "ë²¤ì²˜", "ìŠ¤íƒ€íŠ¸ì—…", "ì°½ì—…", "ì‹œì¥", "ìƒì¸", "ê³¨ëª©ê²½ì œ"],
    "ì œì¡°ì—…": ["ë°˜ë„ì²´", "ìë™ì°¨", "ì¡°ì„ ", "ê¸°ê³„", "í™”í•™", "ì² ê°•", "ì‚°ì—…ë‹¨ì§€"],
    "ì„œë¹„ìŠ¤ì—…": ["ê´€ê´‘", "ìœ í†µ", "ë¬¼ë¥˜", "í”„ëœì°¨ì´ì¦ˆ", "í—¬ìŠ¤ì¼€ì–´", "ë ˆì €", "í˜¸í…”", "ì™¸ì‹"],
    "ë””ì§€í„¸Â·ì •ë³´í†µì‹ ": ["ë””ì§€í„¸", "AI", "ì¸ê³µì§€ëŠ¥", "ë°ì´í„°", "5G", "í´ë¼ìš°ë“œ", "SW", "ICT", "ë¡œë´‡"],
    "ë†ë¦¼ìˆ˜ì‚°": ["ë†ì—…", "ì¶•ì‚°", "ìˆ˜ì‚°", "ë†ë¯¼", "ì–´ë¯¼", "ë†ì´Œ", "ì‹ëŸ‰", "ì‘ë¬¼", "ì–´íš"],
    
    # ğŸ‘¨â€ğŸ‘©â€ğŸ‘§ ë¯¼ìƒÂ·ë³µì§€
    "ë¯¼ìƒÂ·ì„œë¯¼ìƒí™œ": ["ë¬¼ê°€", "ìƒí™œë¹„", "ì›”ì„¸", "ì „ì„¸", "ê³µê³µìš”ê¸ˆ", "ì „ê¸°ì„¸", "êµí†µë¹„", "ìƒê³„"],
    "ë³µì§€Â·ê±´ê°•": ["ë³µì§€", "ê±´ê°•", "ìš”ì–‘", "ì˜ë£Œ", "ê±´ë³´", "ê°„ë³‘", "ì¥ì• ì¸", "ë…¸ì¸", "ì·¨ì•½ê³„ì¸µ"],
    "ë³´ê±´ì˜ë£Œ": ["ë³‘ì›", "ì˜ì‚¬", "ê°„í˜¸ì‚¬", "ë°±ì‹ ", "ì˜ì•½í’ˆ", "ì „ì—¼ë³‘", "ì½”ë¡œë‚˜", "ì˜ë£Œì¸ë ¥"],
    "êµìœ¡": ["êµìœ¡", "í•™êµ", "ëŒ€í•™", "ì…ì‹œ", "ìˆ˜ëŠ¥", "ì²­ë…„", "í•™ì›", "í›ˆë ¨", "ì§ì—…êµìœ¡"],
    "ì²­ë…„Â·ê³ ë ¹": ["ì²­ë…„", "ì²­ë…„ì¸µ", "ì²­ë…„ì •ì±…", "ê³ ë ¹í™”", "ë…¸ì¸", "ì‹œë‹ˆì–´", "ì€í‡´", "ì—°ê¸ˆ"],

    # ğŸ›ï¸ ì •ì¹˜Â·ì œë„
    "ê·œì œÂ·í–‰ì •": ["ê·œì œ", "ì™„í™”", "í—ˆê°€", "ì‹¬ì‚¬", "ì¸í—ˆê°€", "í–‰ì •ì ˆì°¨", "í–‰ì •ê·œì œ", "ë¯¼ì›"],
    "ì •ì¹˜Â·ì„ ê±°": ["ì •ë‹¹", "êµ­íšŒ", "ì˜ì›", "ëŒ€í†µë ¹", "ì´ì„ ", "ì§€ë°©ì„ ê±°", "ì„ ê±°ì œë„"],
    "ë²•ë¥ Â·ì‚¬ë²•": ["ë²•ì›", "íŒê²°", "ê²€ì°°", "ì¬íŒ", "í˜•ì‚¬", "ë¯¼ì‚¬", "í˜•ëŸ‰", "í—Œë²•", "ë²”ì£„"],
    "ì•ˆë³´Â·êµ­ë°©": ["êµ°ì‚¬", "êµ­ë°©", "ì•ˆë³´", "ë¶í•œ", "í›ˆë ¨", "ì „ìŸ", "ì „ëµ", "ë°©ìœ„ë¹„"],

    # ğŸ¨ ì‚¬íšŒÂ·ë¬¸í™”Â·ë¯¸ë””ì–´
    "ë¬¸í™”Â·ì—”í„°í…Œì¸ë¨¼íŠ¸": ["ì˜í™”", "ìŒì•…", "ê³µì—°", "ì½˜í…ì¸ ", "ë“œë¼ë§ˆ", "ì˜ˆëŠ¥", "K-ì½˜í…ì¸ ", "OTT"],
    "ì–¸ë¡ Â·ë¯¸ë””ì–´": ["ì–¸ë¡ ", "ë°©ì†¡", "ì‹ ë¬¸", "ìœ íŠœë¸Œ", "SNS", "ë¯¸ë””ì–´", "í”Œë«í¼"],
    "ì‚¬íšŒë¬¸ì œ": ["ì–‘ê·¹í™”", "ì°¨ë³„", "í˜ì˜¤", "ê°ˆë“±", "ë…¸ìˆ™", "í­ë ¥", "ë²”ì£„", "ì  ë”", "ì´ì£¼ë¯¼"],
    "ì¬ë‚œÂ·ì•ˆì „": ["ì¬ë‚œ", "ì¬í•´", "ì§€ì§„", "í™”ì¬", "í­ìš°", "ì‚°ë¶ˆ", "ê±´ì¶•ë¬¼ë¶•ê´´", "ì•ˆì „ì‚¬ê³ "],

    # ğŸŒ ê¸€ë¡œë²Œ
    "êµ­ì œì •ì„¸": ["ì™¸êµ", "ì •ì„¸", "êµ­ì œ", "ìœ ì—”", "ì¤‘êµ­", "ë¯¸êµ­", "ì¼ë³¸", "ì „ìŸ", "ë¶„ìŸ", "í˜‘ìƒ"],
    "í•´ì™¸ê²½ì œ": ["ê¸€ë¡œë²Œ", "ì„¸ê³„ê²½ì œ", "ë¯¸êµ­ê²½ì œ", "ì¤‘êµ­ê²½ì œ", "í™˜ìœ¨", "ìˆ˜ìš”", "ê³µê¸‰ë§"]
}

# âœ… ê°ì„± ë¶„ì„ ì…ë ¥ ê¸¸ì´ ì œí•œ í•¨ìˆ˜
def truncate_for_sentiment(text, max_length=512):
    inputs = sentiment_tokenizer(text, return_tensors="pt", max_length=max_length, truncation=True)
    return sentiment_tokenizer.decode(inputs["input_ids"][0], skip_special_tokens=True)

# âœ… KoBART ê¸°ë°˜ ìš”ì•½ í•¨ìˆ˜
def summarize_kobart(text):
    input_ids = tokenizer.encode(text, return_tensors="pt", max_length=1024, truncation=True)
    summary_ids = model.generate(
        input_ids,
        max_length=150, min_length=30,
        length_penalty=2.0,
        num_beams=4,
        early_stopping=True
    )
    return tokenizer.decode(summary_ids[0], skip_special_tokens=True)

# âœ… ë¬¸ì¥ì—ì„œ ì‚°ì—… í‚¤ì›Œë“œë³„ ë¬¸ì¥ ì¶”ì¶œ í•¨ìˆ˜
def extract_sector_sentences(text, keyword_dict):
    sector_sentences = defaultdict(list)
    sentences = re.split(r'(?<=[.!?])\s+', text)
    for sentence in sentences:
        for sector, keywords in keyword_dict.items():
            if any(keyword in sentence for keyword in keywords):
                sector_sentences[sector].append(sentence)
    return sector_sentences

# âœ… ì‚°ì—…ë³„ ê°ì„± ë¶„ì„ ì‹¤í–‰ í•¨ìˆ˜
def analyze_sector_sentiments(text, sector_keywords, analyzer):
    result = []
    all_sectors = sector_keywords.keys()
    sectors = extract_sector_sentences(text, sector_keywords)
    for sector in all_sectors:
        sentences = sectors.get(sector, [])
        if sentences:
            combined_text = " ".join(sentences)[:512]
            safe_input = truncate_for_sentiment(combined_text)
            sentiment = analyzer(safe_input)[0]
            result.append({
                "ë¶„ì•¼": sector,
                "ê°ì •": sentiment["label"],
                "ì‹ ë¢°ë„": round(sentiment["score"], 2),
                "ë¬¸ì¥ ìˆ˜": len(sentences),
                "ë‚´ìš©": combined_text
            })
        else:
            result.append({
                "ë¶„ì•¼": sector,
                "ê°ì •": "NEUTRAL",
                "ì‹ ë¢°ë„": 0.1,
                "ë¬¸ì¥ ìˆ˜": 0,
                "ë‚´ìš©": ""
            })
    return result

# âœ… TF-IDF ê¸°ë°˜ ì˜í–¥ë„ ê³„ì‚° í•¨ìˆ˜
def calculate_sector_tfidf(sector_results):
    corpus = [item["ë‚´ìš©"] if item["ë‚´ìš©"] else item["ë¶„ì•¼"] for item in sector_results]
    vectorizer = TfidfVectorizer()
    tfidf_matrix = vectorizer.fit_transform(corpus)
    scores = tfidf_matrix.sum(axis=1).A1
    for i, score in enumerate(scores):
        sector_results[i]["ì˜í–¥ë„"] = round((score + 0.5) * 10, 2)
    return sector_results

# âœ… ê°ì • ë ˆì´ë¸”ì— ë”°ë¥¸ ìƒ‰ìƒê°’ ë³€í™˜ í•¨ìˆ˜
def normalize_score(label, score):
    if label == "POSITIVE":
        return score * 100
    elif label == "NEGATIVE":
        return -score * 100
    else:
        return 0

# âœ… HuggingFace KoAlpaca APIë¥¼ í†µí•œ í”¼ë“œë°± ìƒì„± í•¨ìˆ˜
def query_koalpaca(prompt):
    response = requests.post(API_URL, headers=HEADERS, json={"inputs": prompt})
    try:
        result = response.json()
        if isinstance(result, list) and "generated_text" in result[0]:
            return result[0]["generated_text"]
        elif "error" in result:
            return f"[âš ï¸ ì˜¤ë¥˜] ëª¨ë¸ ì‘ë‹µ ì‹¤íŒ¨: {result['error']}"
        else:
            return "[âš ï¸ ì˜¤ë¥˜] ì˜ˆìƒì¹˜ ëª»í•œ ì‘ë‹µ êµ¬ì¡°ì…ë‹ˆë‹¤."
    except Exception as e:
        return f"[âš ï¸ ì˜ˆì™¸ ë°œìƒ] {str(e)}"

# ê´€ë¦¬ì í”¼ë“œë°± ìƒì„± í•¨ìˆ˜ (GPT-4o í˜¸í™˜)
def generate_feedback_with_openai(summary):
    personas = {
        "ë¶€íšŒì¥": "ëŒ€í•œìƒê³µíšŒì˜ì†Œ ë¶€íšŒì¥ìœ¼ë¡œì„œ, ì •ì±… ë©”ì‹œì§€ì˜ ì„¤ë“ë ¥ê³¼ ê³µê³µì„± ê´€ì ì—ì„œ í‰ê°€í•´ ì£¼ì„¸ìš”.",
        "ì „ë¬´ì´ì‚¬": "ëŒ€í•œìƒê³µíšŒì˜ì†Œ ì „ë¬´ì´ì‚¬ë¡œì„œ, ê¸°ì—… í˜„ì¥ì˜ ì‹¤í–‰ ê°€ëŠ¥ì„±ê³¼ ìˆ˜ìš©ì„± ê´€ì ì—ì„œ í‰ê°€í•´ ì£¼ì„¸ìš”."
    }
    feedbacks = {}
    for name, persona in personas.items():
        prompt = f"{persona}\nì•„ë˜ëŠ” ë³´ë„ìë£Œ ìš”ì•½ì…ë‹ˆë‹¤.\n\n'{summary}'\n\nì´ ë³´ë„ìë£Œì— ëŒ€í•´ ë‹¤ìŒì„ ì‘ì„±í•´ ì£¼ì„¸ìš”:\nâŠ ì´í‰, â‹ ì¢‹ì€ ì , âŒ ë¶€ì¡±í•œ ì , â ê°œì„  ì œì•ˆ."
        try:
            response = openai.chat.completions.create(
                model="gpt-4o",
                messages=[
                    {"role": "system", "content": "ë‹¹ì‹ ì€ ì •ì±… ë³´ê³ ì„œ ê²€í†  ì „ë¬¸ê°€ì…ë‹ˆë‹¤."},
                    {"role": "user", "content": prompt}
                ],
                temperature=0.5
            )
            feedbacks[name] = response.choices[0].message.content.strip()
        except Exception as e:
            feedbacks[name] = f"[âš ï¸ ì˜¤ë¥˜ ë°œìƒ] {str(e)}"
    return feedbacks

# âœ… ì…ë ¥ë€ + ì‹¤í–‰ ë²„íŠ¼
text_input = st.text_area("âœï¸ ë¶„ì„í•  ë³´ë„ìë£Œ ë‚´ìš©ì„ ì…ë ¥í•˜ì„¸ìš”", height=300)

if st.button("ğŸš€ ì‚°ì—…ë³„ ì˜í–¥ë„ + ê°ì„± ë¶„ì„") and text_input.strip():
    with st.spinner("ìš”ì•½ ë° ë¶„ì„ ì¤‘..."):
        summary = summarize_kobart(text_input)
        sector_results = analyze_sector_sentiments(summary, SECTOR_KEYWORDS, sentiment_analyzer)
        sector_results = calculate_sector_tfidf(sector_results)

        viz_data = {
            "ë¶„ì•¼": [],
            "ì˜í–¥ë„": [],
            "ìƒ‰ìƒ": []
        }
        for row in sector_results:
            viz_data["ë¶„ì•¼"].append(row["ë¶„ì•¼"])
            viz_data["ì˜í–¥ë„"].append(row["ì˜í–¥ë„"])
            viz_data["ìƒ‰ìƒ"].append(normalize_score(row["ê°ì •"], row["ì‹ ë¢°ë„"]))

    st.subheader("ğŸ“Œ ë³´ë„ìë£Œ ìš”ì•½")
    st.write(summary)

    fig = px.treemap(
        viz_data,
        path=['ë¶„ì•¼'],
        values='ì˜í–¥ë„',
        color='ìƒ‰ìƒ',
        color_continuous_scale='RdYlGn',
        color_continuous_midpoint=0
    )
    st.subheader("ğŸŒ ì‚°ì—…ë³„ ê°ì„± + ì˜í–¥ë„ ì‹œê°í™”")
    st.plotly_chart(fig, use_container_width=True)

    with st.expander("ğŸ” ì‚°ì—…ë³„ í•˜ìœ„ í‚¤ì›Œë“œ ëª©ë¡"):
        for sector, keywords in SECTOR_KEYWORDS.items():
            st.markdown(f"**{sector}**: `{', '.join(keywords)}`")

    st.subheader("ğŸ§‘â€âš–ï¸ ê²€ìˆ˜ì í”¼ë“œë°±")
    with st.spinner("ë¶€íšŒì¥ë‹˜ê³¼ ì „ë¬´ì´ì‚¬ë‹˜ì˜ ì˜ê²¬ì„ ìƒì„± ì¤‘..."):
        feedbacks = generate_feedback_with_openai(summary)
        for name, opinion in feedbacks.items():
            st.markdown(f"### ğŸ’¬ {name}ë‹˜ì˜ ì˜ê²¬")
            st.info(opinion)